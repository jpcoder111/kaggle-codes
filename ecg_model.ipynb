{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 59093,
          "databundleVersionId": 7469972,
          "sourceType": "competition"
        },
        {
          "sourceId": 7526248,
          "sourceType": "datasetVersion",
          "datasetId": 4308295
        },
        {
          "sourceId": 6127,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4598
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3846.080383,
      "end_time": "2024-01-14T04:20:19.064569",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-01-14T03:16:12.984186",
      "version": "2.4.0"
    },
    "colab": {
      "name": "HMS-HBAC: KerasCV Starter Notebook",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpcoder111/kaggle-codes/blob/main/ecg_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'hms-harmful-brain-activity-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F59093%2F7469972%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240320%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240320T191506Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D822ea0e9b8f1ee293fb7beb55084ba56402f73ff79328775cb9fc3d9dc92ae057b79f7ce79a7b2944c583901e0c00a9434839ad2bc1277b961c43182803dc59c6618a6a4c593d4cbecc9608ce096710b752c42b24a29bf8f98b241f836814b564eb7f0d39a86d54e96350c6875a8b263e5054ef1b82d1622f0641b431402006d3ba2d3b37cd280cf70361a25d7aab92a34f18d5b078e2de8379e49cb7d209dbe62f9a6f3e9a26833ddda7b35254268577accea1884ff1aae2b6fa1c6879243e9853a5942dfa5ff4b54b768cdcf170b1d10568f4e2e41a8813d47cd876db6536d3cc989d996a33c4e8e521b070957fefabe82f4ac4e1911e98d941eca02d504ac,kerasv3-lib-ds:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4308295%2F7526248%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240320%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240320T191506Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D62163abd1aa28147da91a2cec4f6b486c678a5c81bed24834cf5083eb70b12ec46b8f8e0983a1c8910947871be6978c323f59a10d1c300e67c1b2780c2e7eec6e19cd2dcefb19683b32326da1cd4cdefefee2c2ae7d925a35294af2da347b37bd4c5d153ff140d4e4bdf0da3b2eff2334be8526298b7d59c19be527e085aa7c95983df882aa5917cf416d88296a2d26a4d15f08f857bf50e7fc65cfe98fb7ae9b15a41518e3542fa0c410e7c3e5db8f5f8fa581c4b3b01c88be03fadb1dee3e221af31ba91b173504b40c2ed466d0e1de987257d337dc0e9015ce6a34362f28cebf1f33ffdcfa703cb2d52dbee874373ca36450e7a9c2b10ddb30e6fd00bac16,efficientnetv2/keras/efficientnetv2_b2_imagenet/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4598%2F6127%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240320%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240320T191506Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db348a19faa17ffb0fc13044cae628a8301a3855e11e5763bb8be5a0da8cce764fe78fb38c610e1aff4f65a535d1ab268d7638e89df90c81af33e024376d0426d68cde3fb99d2742483e587224dbc09101ecef5f553c5b4558426f2c9f88e7b11ecaf223a0eba7e0105b43b06fdcd1ac7f691a653da2a3b596c033f6add00494ec521b716b8bfd5cb3605d017e817c1c9e66047656d32202dee2e9ced55c59b96a5e910c5d0d9f5dc6f3a4a99d53df44904f921ea541dc3e4e2f39a9904dfe1646da5565bb2eba4ba3704e7195dd82b870ca9062998e9f9f3b82d133d83510047f8c04a9d3672032002d10dcbd8cc3364cb6f39da6c6663b6413ea5d4c686e998'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "nRIhlpoeSepj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
        "This starter notebook is provided by the Keras team.</center>"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T05:24:31.308329Z",
          "iopub.status.busy": "2024-01-10T05:24:31.307595Z",
          "iopub.status.idle": "2024-01-10T05:24:31.313088Z",
          "shell.execute_reply": "2024-01-10T05:24:31.312113Z",
          "shell.execute_reply.started": "2024-01-10T05:24:31.308287Z"
        },
        "papermill": {
          "duration": 0.011755,
          "end_time": "2024-01-14T03:16:16.447481",
          "exception": false,
          "start_time": "2024-01-14T03:16:16.435726",
          "status": "completed"
        },
        "tags": [],
        "id": "H-TDujK9Sepl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HMS - Harmful Brain Activity Classification with [KerasCV](https://github.com/keras-team/keras-cv) and [Keras](https://github.com/keras-team/keras)\n",
        "\n",
        "> The objective of this competition is to classify seizures and other patterns of harmful brain activity in critically ill patients\n",
        "\n",
        "This notebook guides you through the process of training and inferring a Deep Learning model, specifically EfficientNetV2, using KerasCV on the competition dataset. Specificaclly, this notebook uses spectrogram of the eeg data to classify the patterns.\n",
        "\n",
        "Fun fact: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Utilizing KerasCV and Keras allows us to choose our preferred backend. Explore more details on [Keras](https://keras.io/keras_core/announcement/).\n",
        "\n",
        "In this notebook, you will learn:\n",
        "\n",
        "* Loading the data efficiently using [`tf.data`](https://www.tensorflow.org/guide/data).\n",
        "* Creating the model using KerasCV presets.\n",
        "* Training the model.\n",
        "* Inference and Submission on test data.\n",
        "\n",
        "**Note**: For a more in-depth understanding of KerasCV, refer to the [KerasCV guides](https://keras.io/guides/keras_cv/)."
      ],
      "metadata": {
        "id": "2dV1TjbZSepm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠 | Install Libraries  \n",
        "\n",
        "Since internet access is **disabled** during inference, we cannot install libraries in the usual `!pip install <lib_name>` manner. Instead, we need to install libraries from local files. In the following cell, we will install libraries from our local files. The installation code stays very similar - we just use the `filepath` instead of the `filename` of the library. So now the code is `!pip install <local_filepath>`.\n",
        "\n",
        "> The `filepath` of these local libraries look quite complicated, but don't be intimidated! Also `--no-deps` argument ensures that we are not installing any additional libraries."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011416,
          "end_time": "2024-01-14T03:16:16.470167",
          "exception": false,
          "start_time": "2024-01-14T03:16:16.458751",
          "status": "completed"
        },
        "tags": [],
        "id": "zX07MxNOSepm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q /kaggle/input/kerasv3-lib-ds/keras_cv-0.8.2-py3-none-any.whl --no-deps\n",
        "!pip install -q /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n",
        "!pip install -q /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl --no-deps"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T08:09:58.471669Z",
          "iopub.execute_input": "2024-02-01T08:09:58.472046Z",
          "iopub.status.idle": "2024-02-01T08:11:45.612735Z",
          "shell.execute_reply.started": "2024-02-01T08:09:58.472001Z",
          "shell.execute_reply": "2024-02-01T08:11:45.611628Z"
        },
        "trusted": true,
        "id": "JIPT8C5wSepm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 | Import Libraries"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010878,
          "end_time": "2024-01-14T03:17:49.510159",
          "exception": false,
          "start_time": "2024-01-14T03:17:49.499281",
          "status": "completed"
        },
        "tags": [],
        "id": "ydRG0M8vSepm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
        "\n",
        "import keras_cv\n",
        "import keras\n",
        "from keras import ops\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "papermill": {
          "duration": 10.671979,
          "end_time": "2024-01-14T03:18:00.193134",
          "exception": false,
          "start_time": "2024-01-14T03:17:49.521155",
          "status": "completed"
        },
        "tags": [],
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:45.614724Z",
          "iopub.execute_input": "2024-02-01T08:11:45.615026Z",
          "iopub.status.idle": "2024-02-01T08:11:56.499875Z",
          "shell.execute_reply.started": "2024-02-01T08:11:45.615Z",
          "shell.execute_reply": "2024-02-01T08:11:56.499055Z"
        },
        "trusted": true,
        "id": "N5_85EnxSepn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Versions"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010958,
          "end_time": "2024-01-14T03:18:00.215704",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.204746",
          "status": "completed"
        },
        "tags": [],
        "id": "tlPG3DItSepn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"Keras:\", keras.__version__)\n",
        "print(\"KerasCV:\", keras_cv.__version__)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019435,
          "end_time": "2024-01-14T03:18:00.246368",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.226933",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:56.500924Z",
          "iopub.execute_input": "2024-02-01T08:11:56.501441Z",
          "iopub.status.idle": "2024-02-01T08:11:56.506615Z",
          "shell.execute_reply.started": "2024-02-01T08:11:56.501416Z",
          "shell.execute_reply": "2024-02-01T08:11:56.505678Z"
        },
        "trusted": true,
        "id": "BczN-iygSepn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ | Configuration"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010922,
          "end_time": "2024-01-14T03:18:00.26855",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.257628",
          "status": "completed"
        },
        "tags": [],
        "id": "i2pe1s6rSepn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    verbose = 1  # Verbosity\n",
        "    seed = 42  # Random seed\n",
        "    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n",
        "    image_size = [400, 300]  # Input image size\n",
        "    epochs = 13 # Training epochs\n",
        "    batch_size = 64  # Batch size\n",
        "    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
        "    drop_remainder = True  # Drop incomplete batches\n",
        "    num_classes = 6 # Number of classes in the dataset\n",
        "    fold = 0 # Which fold to set as validation data\n",
        "    class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n",
        "    label2name = dict(enumerate(class_names))\n",
        "    name2label = {v:k for k, v in label2name.items()}"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018795,
          "end_time": "2024-01-14T03:18:00.298534",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.279739",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:56.509498Z",
          "iopub.execute_input": "2024-02-01T08:11:56.510141Z",
          "iopub.status.idle": "2024-02-01T08:11:56.526461Z",
          "shell.execute_reply.started": "2024-02-01T08:11:56.51011Z",
          "shell.execute_reply": "2024-02-01T08:11:56.525619Z"
        },
        "trusted": true,
        "id": "mgkxzQaLSepn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ♻️ | Reproducibility\n",
        "Sets value for random seed to produce similar result in each run."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010907,
          "end_time": "2024-01-14T03:18:00.32063",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.309723",
          "status": "completed"
        },
        "tags": [],
        "id": "yf8CS59JSepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(CFG.seed)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018371,
          "end_time": "2024-01-14T03:18:00.350074",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.331703",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:56.527613Z",
          "iopub.execute_input": "2024-02-01T08:11:56.527893Z",
          "iopub.status.idle": "2024-02-01T08:11:56.536142Z",
          "shell.execute_reply.started": "2024-02-01T08:11:56.527869Z",
          "shell.execute_reply": "2024-02-01T08:11:56.535296Z"
        },
        "trusted": true,
        "id": "aV1nzzPySepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📁 | Dataset Path"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010888,
          "end_time": "2024-01-14T03:18:00.372053",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.361165",
          "status": "completed"
        },
        "tags": [],
        "id": "rJkln-MmSepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/kaggle/input/hms-harmful-brain-activity-classification\"\n",
        "\n",
        "SPEC_DIR = \"/tmp/dataset/hms-hbac\"\n",
        "os.makedirs(SPEC_DIR+'/train_spectrograms', exist_ok=True)\n",
        "os.makedirs(SPEC_DIR+'/test_spectrograms', exist_ok=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017704,
          "end_time": "2024-01-14T03:18:00.400852",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.383148",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:56.537196Z",
          "iopub.execute_input": "2024-02-01T08:11:56.5375Z",
          "iopub.status.idle": "2024-02-01T08:11:56.548849Z",
          "shell.execute_reply.started": "2024-02-01T08:11:56.537478Z",
          "shell.execute_reply": "2024-02-01T08:11:56.548017Z"
        },
        "trusted": true,
        "id": "BPGT3dxASepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 | Meta Data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011434,
          "end_time": "2024-01-14T03:18:00.472401",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.460967",
          "status": "completed"
        },
        "tags": [],
        "id": "Ibc6-4-lSepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train + Valid\n",
        "df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
        "df['eeg_path'] = f'{BASE_PATH}/train_eegs/'+df['eeg_id'].astype(str)+'.parquet'\n",
        "df['spec_path'] = f'{BASE_PATH}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.parquet'\n",
        "df['spec2_path'] = f'{SPEC_DIR}/train_spectrograms/'+df['spectrogram_id'].astype(str)+'.npy'\n",
        "df['class_name'] = df.expert_consensus.copy()\n",
        "df['class_label'] = df.expert_consensus.map(CFG.name2label)\n",
        "display(df.head(2))\n",
        "\n",
        "# Test\n",
        "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
        "test_df['eeg_path'] = f'{BASE_PATH}/test_eegs/'+test_df['eeg_id'].astype(str)+'.parquet'\n",
        "test_df['spec_path'] = f'{BASE_PATH}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.parquet'\n",
        "test_df['spec2_path'] = f'{SPEC_DIR}/test_spectrograms/'+test_df['spectrogram_id'].astype(str)+'.npy'\n",
        "display(test_df.head(2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:56.549878Z",
          "iopub.execute_input": "2024-02-01T08:11:56.550129Z",
          "iopub.status.idle": "2024-02-01T08:11:57.142269Z",
          "shell.execute_reply.started": "2024-02-01T08:11:56.550108Z",
          "shell.execute_reply": "2024-02-01T08:11:57.141377Z"
        },
        "trusted": true,
        "id": "fujg_z9wSepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert `.parquet` to `.npy`\n",
        "\n",
        "To facilitate easier data loading, we will convert the EEG spectrograms from `parquet` to `npy` format. This process involves saving the spectrogram data, and since the content of the files remains the same, no significant changes are made.\n",
        "\n",
        "> It's worth noting that the `time` column is excluded, as it is not part of the spectrogram."
      ],
      "metadata": {
        "id": "evygJbs4Sepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process a single eeg_id\n",
        "def process_spec(spec_id, split=\"train\"):\n",
        "    spec_path = f\"{BASE_PATH}/{split}_spectrograms/{spec_id}.parquet\"\n",
        "    spec = pd.read_parquet(spec_path)\n",
        "    spec = spec.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n",
        "    spec = spec.astype(\"float32\")\n",
        "    np.save(f\"{SPEC_DIR}/{split}_spectrograms/{spec_id}.npy\", spec)\n",
        "\n",
        "# Get unique spec_ids of train and valid data\n",
        "spec_ids = df[\"spectrogram_id\"].unique()\n",
        "\n",
        "# Parallelize the processing using joblib for training data\n",
        "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
        "    joblib.delayed(process_spec)(spec_id, \"train\")\n",
        "    for spec_id in tqdm(spec_ids, total=len(spec_ids))\n",
        ")\n",
        "\n",
        "# Get unique spec_ids of test data\n",
        "test_spec_ids = test_df[\"spectrogram_id\"].unique()\n",
        "\n",
        "# Parallelize the processing using joblib for test data\n",
        "_ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
        "    joblib.delayed(process_spec)(spec_id, \"test\")\n",
        "    for spec_id in tqdm(test_spec_ids, total=len(test_spec_ids))\n",
        ")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.86264,
          "end_time": "2024-01-14T03:18:01.346487",
          "exception": false,
          "start_time": "2024-01-14T03:18:00.483847",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:11:57.143405Z",
          "iopub.execute_input": "2024-02-01T08:11:57.143667Z",
          "iopub.status.idle": "2024-02-01T08:14:58.676582Z",
          "shell.execute_reply.started": "2024-02-01T08:11:57.143644Z",
          "shell.execute_reply": "2024-02-01T08:14:58.675463Z"
        },
        "trusted": true,
        "id": "2ypsLCQjSepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍚 | DataLoader\n",
        "\n",
        "This DataLoader first reads `npy` spectrogram files and extracts labeled subsamples using specified `offset` values. Then, it converts the spectrogram data into `log spectrogram` and applies the popular signal augmentation `MixUp`.\n",
        "\n",
        "> Note that, we are converting the mono channel signal to a 3-channel signal for using \"ImageNet\" weights of pretrained model."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011843,
          "end_time": "2024-01-14T03:18:01.457956",
          "exception": false,
          "start_time": "2024-01-14T03:18:01.446113",
          "status": "completed"
        },
        "tags": [],
        "id": "QPjQ5hpVSepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_augmenter(dim=CFG.image_size):\n",
        "    augmenters = [\n",
        "        keras_cv.layers.MixUp(alpha=2.0),\n",
        "        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0),\n",
        "                                     width_factor=(0.06, 0.1)), # freq-masking\n",
        "        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1),\n",
        "                                     width_factor=(1.0, 1.0)), # time-masking\n",
        "    ]\n",
        "\n",
        "    def augment(img, label):\n",
        "        data = {\"images\":img, \"labels\":label}\n",
        "        for augmenter in augmenters:\n",
        "            if tf.random.uniform([]) < 0.5:\n",
        "                data = augmenter(data, training=True)\n",
        "        return data[\"images\"], data[\"labels\"]\n",
        "\n",
        "    return augment\n",
        "\n",
        "\n",
        "def build_decoder(with_labels=True, target_size=CFG.image_size, dtype=32):\n",
        "    def decode_signal(path, offset=None):\n",
        "        # Read .npy files and process the signal\n",
        "        file_bytes = tf.io.read_file(path)\n",
        "        sig = tf.io.decode_raw(file_bytes, tf.float32)\n",
        "        sig = sig[1024//dtype:]  # Remove header tag\n",
        "        sig = tf.reshape(sig, [400, -1])\n",
        "\n",
        "        # Extract labeled subsample from full spectrogram using \"offset\"\n",
        "        if offset is not None:\n",
        "            offset = offset // 2  # Only odd values are given\n",
        "            sig = sig[:, offset:offset+300]\n",
        "\n",
        "            # Pad spectrogram to ensure the same input shape of [400, 300]\n",
        "            pad_size = tf.math.maximum(0, 300 - tf.shape(sig)[1])\n",
        "            sig = tf.pad(sig, [[0, 0], [0, pad_size]])\n",
        "            sig = tf.reshape(sig, [400, 300])\n",
        "\n",
        "        # Log spectrogram\n",
        "        sig = tf.clip_by_value(sig, tf.math.exp(-4.0), tf.math.exp(8.0)) # avoid 0 in log\n",
        "        sig = tf.math.log(sig)\n",
        "\n",
        "        # Normalize spectrogram\n",
        "        sig -= tf.math.reduce_mean(sig)\n",
        "        sig /= tf.math.reduce_std(sig) + 1e-6\n",
        "\n",
        "        # Mono channel to 3 channels to use \"ImageNet\" weights\n",
        "        sig = tf.tile(sig[..., None], [1, 1, 3])\n",
        "        return sig\n",
        "\n",
        "    def decode_label(label):\n",
        "        label = tf.one_hot(label, CFG.num_classes)\n",
        "        label = tf.cast(label, tf.float32)\n",
        "        label = tf.reshape(label, [CFG.num_classes])\n",
        "        return label\n",
        "\n",
        "    def decode_with_labels(path, offset=None, label=None):\n",
        "        sig = decode_signal(path, offset)\n",
        "        label = decode_label(label)\n",
        "        return (sig, label)\n",
        "\n",
        "    return decode_with_labels if with_labels else decode_signal\n",
        "\n",
        "\n",
        "def build_dataset(paths, offsets=None, labels=None, batch_size=32, cache=True,\n",
        "                  decode_fn=None, augment_fn=None,\n",
        "                  augment=False, repeat=True, shuffle=1024,\n",
        "                  cache_dir=\"\", drop_remainder=False):\n",
        "    if cache_dir != \"\" and cache is True:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    if decode_fn is None:\n",
        "        decode_fn = build_decoder(labels is not None)\n",
        "\n",
        "    if augment_fn is None:\n",
        "        augment_fn = build_augmenter()\n",
        "\n",
        "    AUTO = tf.data.experimental.AUTOTUNE\n",
        "    slices = (paths, offsets) if labels is None else (paths, offsets, labels)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
        "    ds = ds.cache(cache_dir) if cache else ds\n",
        "    ds = ds.repeat() if repeat else ds\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
        "        opt = tf.data.Options()\n",
        "        opt.experimental_deterministic = False\n",
        "        ds = ds.with_options(opt)\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
        "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
        "    ds = ds.prefetch(AUTO)\n",
        "    return ds"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.039133,
          "end_time": "2024-01-14T03:18:01.509017",
          "exception": false,
          "start_time": "2024-01-14T03:18:01.469884",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:17:27.278733Z",
          "iopub.execute_input": "2024-02-01T08:17:27.279117Z",
          "iopub.status.idle": "2024-02-01T08:17:27.300354Z",
          "shell.execute_reply.started": "2024-02-01T08:17:27.279091Z",
          "shell.execute_reply": "2024-02-01T08:17:27.299141Z"
        },
        "trusted": true,
        "id": "-ta0Iu1aSepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔪 | Data Split\n",
        "\n",
        "In the following code snippet, the data is divided into `5` folds. Note that, the `groups` argument is used to prevent any overlap of patients between the training and validation sets, thus avoiding potential **data leakage** issues. Additionally, each split is stratified based on the `class_label`, ensuring a uniform distribution of class labels in each fold."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012174,
          "end_time": "2024-01-14T03:18:01.538524",
          "exception": false,
          "start_time": "2024-01-14T03:18:01.52635",
          "status": "completed"
        },
        "tags": [],
        "id": "kegbt-g2Sepo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n",
        "\n",
        "df[\"fold\"] = -1\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "for fold, (train_idx, valid_idx) in enumerate(\n",
        "    sgkf.split(df, y=df[\"class_label\"], groups=df[\"patient_id\"])\n",
        "):\n",
        "    df.loc[valid_idx, \"fold\"] = fold\n",
        "df.groupby([\"fold\", \"class_name\"])[[\"eeg_id\"]].count().T"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.037496,
          "end_time": "2024-01-14T03:18:01.587924",
          "exception": false,
          "start_time": "2024-01-14T03:18:01.550428",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-01T08:14:58.702622Z",
          "iopub.execute_input": "2024-02-01T08:14:58.702904Z",
          "iopub.status.idle": "2024-02-01T08:15:01.163856Z",
          "shell.execute_reply.started": "2024-02-01T08:14:58.702882Z",
          "shell.execute_reply": "2024-02-01T08:15:01.162898Z"
        },
        "trusted": true,
        "id": "mkTt4lqJSepo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Train & Valid Dataset\n",
        "\n",
        "Only first sample for each `spectrogram_id` is used in order to keep the dataset size managable. Feel free to train on full data."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011875,
          "end_time": "2024-01-14T03:18:01.611955",
          "exception": false,
          "start_time": "2024-01-14T03:18:01.60008",
          "status": "completed"
        },
        "tags": [],
        "id": "b6Czs4w8Sepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from full data\n",
        "sample_df = df.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\n",
        "train_df = sample_df[sample_df.fold != CFG.fold]\n",
        "valid_df = sample_df[sample_df.fold == CFG.fold]\n",
        "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
        "\n",
        "# Train\n",
        "train_paths = train_df.spec2_path.values\n",
        "train_offsets = train_df.spectrogram_label_offset_seconds.values.astype(int)\n",
        "train_labels = train_df.class_label.values\n",
        "train_ds = build_dataset(train_paths, train_offsets, train_labels, batch_size=CFG.batch_size,\n",
        "                         repeat=True, shuffle=True, augment=True, cache=True)\n",
        "\n",
        "# Valid\n",
        "valid_paths = valid_df.spec2_path.values\n",
        "valid_offsets = valid_df.spectrogram_label_offset_seconds.values.astype(int)\n",
        "valid_labels = valid_df.class_label.values\n",
        "valid_ds = build_dataset(valid_paths, valid_offsets, valid_labels, batch_size=CFG.batch_size,\n",
        "                         repeat=False, shuffle=False, augment=False, cache=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T08:17:29.652278Z",
          "iopub.execute_input": "2024-02-01T08:17:29.652748Z",
          "iopub.status.idle": "2024-02-01T08:17:30.626157Z",
          "shell.execute_reply.started": "2024-02-01T08:17:29.652709Z",
          "shell.execute_reply": "2024-02-01T08:17:30.625103Z"
        },
        "trusted": true,
        "id": "FIzL-cUWSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Check\n",
        "\n",
        "Let's visualize some samples from the dataset."
      ],
      "metadata": {
        "id": "z_un_MgWSepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, tars = next(iter(train_ds))\n",
        "\n",
        "num_imgs = 8\n",
        "plt.figure(figsize=(4*4, num_imgs//4*5))\n",
        "for i in range(num_imgs):\n",
        "    plt.subplot(num_imgs//4, 4, i + 1)\n",
        "    img = imgs[i].numpy()[...,0]  # Adjust as per your image data format\n",
        "    img -= img.min()\n",
        "    img /= img.max() + 1e-4\n",
        "    tar = CFG.label2name[np.argmax(tars[i].numpy())]\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Target: {tar}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-01T08:17:33.806036Z",
          "iopub.execute_input": "2024-02-01T08:17:33.806415Z",
          "iopub.status.idle": "2024-02-01T08:17:37.720083Z",
          "shell.execute_reply.started": "2024-02-01T08:17:33.806385Z",
          "shell.execute_reply": "2024-02-01T08:17:37.718626Z"
        },
        "trusted": true,
        "id": "2Khp6VuaSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 | Loss & Metric\n",
        "\n",
        "The evaluation metric in this competition is **KL Divergence**, defined as,\n",
        "\n",
        "$$\n",
        "D_{\\text{KL}}(P \\parallel Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $P$ is the true distribution.\n",
        "- $Q$ is the predicted distribution.\n",
        "\n",
        "Interestingly, as KL Divergence is differentiable, we can directly use it as our loss function. Thus, we don't need to use a third-party metric like **Accuracy** to evaluate our model. Therefore, `valid_loss` can stand alone as an indicator for our evaluation. In keras, we already have impelementation for KL Divergence loss so we only need to import it."
      ],
      "metadata": {
        "id": "smQL72BeSepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOSS = keras.losses.KLDivergence()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:23.763944Z",
          "iopub.execute_input": "2024-01-21T08:11:23.764307Z",
          "iopub.status.idle": "2024-01-21T08:11:23.770792Z",
          "shell.execute_reply.started": "2024-01-21T08:11:23.764273Z",
          "shell.execute_reply": "2024-01-21T08:11:23.768924Z"
        },
        "trusted": true,
        "id": "W8K6YRqHSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 | Modeling\n",
        "\n",
        "This notebook uses the `EfficientNetV2 B2` from KerasCV's collection of pretrained models. To explore other models, simply modify the `preset` in the `CFG` (config). Check the [KerasCV website](https://keras.io/api/keras_cv/models/tasks/image_classifier/) for a list of available pretrained models."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016849,
          "end_time": "2024-01-14T03:18:38.613991",
          "exception": false,
          "start_time": "2024-01-14T03:18:38.597142",
          "status": "completed"
        },
        "tags": [],
        "id": "6XorZ-krSepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Classifier\n",
        "model = keras_cv.models.ImageClassifier.from_preset(\n",
        "    CFG.preset, num_classes=CFG.num_classes\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=LOSS)\n",
        "\n",
        "# Model Sumamry\n",
        "model.summary()"
      ],
      "metadata": {
        "papermill": {
          "duration": 10.446166,
          "end_time": "2024-01-14T03:18:49.186176",
          "exception": false,
          "start_time": "2024-01-14T03:18:38.74001",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:23.771871Z",
          "iopub.execute_input": "2024-01-21T08:11:23.772146Z",
          "iopub.status.idle": "2024-01-21T08:11:47.907132Z",
          "shell.execute_reply.started": "2024-01-21T08:11:23.77212Z",
          "shell.execute_reply": "2024-01-21T08:11:47.906227Z"
        },
        "trusted": true,
        "id": "VEfk1qxfSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚓ | LR Schedule\n",
        "\n",
        "A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.016209,
          "end_time": "2024-01-14T03:18:49.21924",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.203031",
          "status": "completed"
        },
        "tags": [],
        "id": "v4lmcFc2Sepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
        "    lr_start, lr_max, lr_min = 5e-5, 6e-6 * batch_size, 1e-5\n",
        "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
        "\n",
        "    def lrfn(epoch):  # Learning rate update function\n",
        "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
        "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
        "        elif mode == 'cos':\n",
        "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        return lr\n",
        "\n",
        "    if plot:  # Plot lr curve if plot is True\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
        "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
        "        plt.title('LR Scheduler')\n",
        "        plt.show()\n",
        "\n",
        "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.028945,
          "end_time": "2024-01-14T03:18:49.264535",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.23559",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:47.908346Z",
          "iopub.execute_input": "2024-01-21T08:11:47.908637Z",
          "iopub.status.idle": "2024-01-21T08:11:47.918578Z",
          "shell.execute_reply.started": "2024-01-21T08:11:47.90861Z",
          "shell.execute_reply": "2024-01-21T08:11:47.917684Z"
        },
        "trusted": true,
        "id": "m1iZqzQ1Sepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_cb = get_lr_callback(CFG.batch_size, mode=CFG.lr_mode, plot=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.297147,
          "end_time": "2024-01-14T03:18:49.578089",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.280942",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:47.919711Z",
          "iopub.execute_input": "2024-01-21T08:11:47.920059Z",
          "iopub.status.idle": "2024-01-21T08:11:48.208939Z",
          "shell.execute_reply.started": "2024-01-21T08:11:47.920032Z",
          "shell.execute_reply": "2024-01-21T08:11:48.207965Z"
        },
        "trusted": true,
        "id": "d_7UMJJ1Sepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💾 | Model Checkpointing"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017199,
          "end_time": "2024-01-14T03:18:49.613648",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.596449",
          "status": "completed"
        },
        "tags": [],
        "id": "D2PCfp-bSepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n",
        "                                         monitor='val_loss',\n",
        "                                         save_best_only=True,\n",
        "                                         save_weights_only=False,\n",
        "                                         mode='min')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.024529,
          "end_time": "2024-01-14T03:18:49.655708",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.631179",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:48.210251Z",
          "iopub.execute_input": "2024-01-21T08:11:48.210551Z",
          "iopub.status.idle": "2024-01-21T08:11:48.21519Z",
          "shell.execute_reply.started": "2024-01-21T08:11:48.210525Z",
          "shell.execute_reply": "2024-01-21T08:11:48.214326Z"
        },
        "trusted": true,
        "id": "mp4kTfLCSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚂 | Training"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01671,
          "end_time": "2024-01-14T03:18:49.689354",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.672644",
          "status": "completed"
        },
        "tags": [],
        "id": "9Xu6beZiSepp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=CFG.epochs,\n",
        "    callbacks=[lr_cb, ckpt_cb],\n",
        "    steps_per_epoch=len(train_df)//CFG.batch_size,\n",
        "    validation_data=valid_ds,\n",
        "    verbose=CFG.verbose\n",
        ")"
      ],
      "metadata": {
        "papermill": {
          "duration": 3374.692199,
          "end_time": "2024-01-14T04:15:04.398389",
          "exception": false,
          "start_time": "2024-01-14T03:18:49.70619",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:11:48.216319Z",
          "iopub.execute_input": "2024-01-21T08:11:48.216638Z",
          "iopub.status.idle": "2024-01-21T08:26:11.991931Z",
          "shell.execute_reply.started": "2024-01-21T08:11:48.216588Z",
          "shell.execute_reply": "2024-01-21T08:26:11.990786Z"
        },
        "trusted": true,
        "id": "iFHNLn5nSepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 | Prediction"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.693309,
          "end_time": "2024-01-14T04:15:05.731839",
          "exception": false,
          "start_time": "2024-01-14T04:15:05.03853",
          "status": "completed"
        },
        "tags": [],
        "id": "JFZnV9WvSepp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Best Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.632183,
          "end_time": "2024-01-14T04:15:06.991143",
          "exception": false,
          "start_time": "2024-01-14T04:15:06.35896",
          "status": "completed"
        },
        "tags": [],
        "id": "cmE7fCTPSept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"best_model.keras\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 20.428261,
          "end_time": "2024-01-14T04:15:28.044401",
          "exception": false,
          "start_time": "2024-01-14T04:15:07.61614",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-01-21T08:26:11.994138Z",
          "iopub.execute_input": "2024-01-21T08:26:11.994508Z",
          "iopub.status.idle": "2024-01-21T08:26:19.318291Z",
          "shell.execute_reply.started": "2024-01-21T08:26:11.994479Z",
          "shell.execute_reply": "2024-01-21T08:26:19.317485Z"
        },
        "trusted": true,
        "id": "5JVnsxQXSept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Test Dataset"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.703901,
          "end_time": "2024-01-14T04:20:09.745279",
          "exception": false,
          "start_time": "2024-01-14T04:20:09.041378",
          "status": "completed"
        },
        "tags": [],
        "id": "pdUFlDuVSept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths = test_df.spec2_path.values\n",
        "test_ds = build_dataset(test_paths, batch_size=min(CFG.batch_size, len(test_df)),\n",
        "                         repeat=False, shuffle=False, cache=False, augment=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T08:26:19.320221Z",
          "iopub.execute_input": "2024-01-21T08:26:19.320511Z",
          "iopub.status.idle": "2024-01-21T08:26:19.366196Z",
          "shell.execute_reply.started": "2024-01-21T08:26:19.320486Z",
          "shell.execute_reply": "2024-01-21T08:26:19.365433Z"
        },
        "trusted": true,
        "id": "idFS1bQ6Sept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "oz4ZkC2vSept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_ds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T08:26:19.367379Z",
          "iopub.execute_input": "2024-01-21T08:26:19.367983Z",
          "iopub.status.idle": "2024-01-21T08:26:44.828629Z",
          "shell.execute_reply.started": "2024-01-21T08:26:19.367955Z",
          "shell.execute_reply": "2024-01-21T08:26:44.827887Z"
        },
        "trusted": true,
        "id": "ShphUU0vSept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📩 | Submission"
      ],
      "metadata": {
        "id": "TDuYlr0qSept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df = test_df[[\"eeg_id\"]].copy()\n",
        "target_cols = [x.lower()+'_vote' for x in CFG.class_names]\n",
        "pred_df[target_cols] = preds.tolist()\n",
        "\n",
        "sub_df = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\n",
        "sub_df = sub_df[[\"eeg_id\"]].copy()\n",
        "sub_df = sub_df.merge(pred_df, on=\"eeg_id\", how=\"left\")\n",
        "sub_df.to_csv(\"submission.csv\", index=False)\n",
        "sub_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T08:26:44.830108Z",
          "iopub.execute_input": "2024-01-21T08:26:44.830386Z",
          "iopub.status.idle": "2024-01-21T08:26:44.873794Z",
          "shell.execute_reply.started": "2024-01-21T08:26:44.830361Z",
          "shell.execute_reply": "2024-01-21T08:26:44.872998Z"
        },
        "trusted": true,
        "id": "V_aGmBE9Sepu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 | Reference\n",
        "* [HMS-HBAC: ResNet34d Baseline [Training]](https://www.kaggle.com/code/ttahara/hms-hbac-resnet34d-baseline-training)\n",
        "* [EfficientNetB2 Starter - [LB 0.57]](https://www.kaggle.com/code/cdeotte/efficientnetb2-starter-lb-0-57)"
      ],
      "metadata": {
        "id": "DOATRpE3Sepu"
      }
    }
  ]
}